<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>控制：三个坐标系与运动学方程</title>
    <link href="/2024/07/04/%E6%8E%A7%E5%88%B6%EF%BC%9A%E4%B8%89%E4%B8%AA%E5%9D%90%E6%A0%87%E7%B3%BB%E4%B8%8E%E8%BF%90%E5%8A%A8%E5%AD%A6%E6%96%B9%E7%A8%8B/"/>
    <url>/2024/07/04/%E6%8E%A7%E5%88%B6%EF%BC%9A%E4%B8%89%E4%B8%AA%E5%9D%90%E6%A0%87%E7%B3%BB%E4%B8%8E%E8%BF%90%E5%8A%A8%E5%AD%A6%E6%96%B9%E7%A8%8B/</url>
    
    <content type="html"><![CDATA[<h2 id="apollo-控制算法">Apollo 控制算法</h2><p>采用 PID + LQR 算法，大多数选择MPC， 但是MPC的计算量太大</p><p>默认已经有一条路径规划</p><h3 id="控制">控制</h3><ol type="1"><li>油门/刹车:arrow_right:力:arrow_right:加速度:arrow_right:速度:arrow_right:位置</li><li>方向盘:arrow_right:前轮转角:arrow_right:横向位移and航向角</li></ol><h3 id="三个坐标系">三个坐标系</h3><ol type="1"><li>绝对坐标系，大地坐标系(X-Y)</li><li>车身坐标系(x-y)</li><li>自然坐标系(Frenet坐标系)</li></ol><blockquote><p>现在一律采用右手系，符合右手拇指指向 X 轴，食指指向 Y 轴，中指指向 Z 轴,算是符合右手系。反之是左手系，一般数学物理类是右手系，计算机或者视觉专业是左手系。</p></blockquote><p>一般对车进行建模分析的时候都看成自行车模型，把四个轮子变成两个轮子，认为车是完全的左右对称的，并且左右轮距相对于轨迹来说可以忽略不计。进行简化</p><h3 id="速度计算">速度计算</h3><p>用理论力学的<font color="red">顺心法</font></p><figure><img src="/2024/07/04/%E6%8E%A7%E5%88%B6%EF%BC%9A%E4%B8%89%E4%B8%AA%E5%9D%90%E6%A0%87%E7%B3%BB%E4%B8%8E%E8%BF%90%E5%8A%A8%E5%AD%A6%E6%96%B9%E7%A8%8B/image-20240404235128726.png" alt><figcaption>image-20240404235128726</figcaption></figure><p>垂线的方向就是质心速度的方向，车的质心速度方向一般和车的轴线方向一般不会重合，车的轴线方向和大地坐标系的 X 方向一般也不是重合的。所以要定义两个角度</p><p><span class="math inline">\(\phi\)</span> 横摆角:车的轴线与<span class="math inline">\(\overrightarrow{X}\)</span>方向的夹角，<em>绝对坐标系下</em></p><p><span class="math inline">\(\beta\)</span>质心侧偏角：质心速度与<span class="math inline">\(\overrightarrow{x}\)</span>方向的夹角，<em>车身坐标系下</em></p><p><span class="math inline">\(\phi+\beta\)</span>航向角：质心速度与<span class="math inline">\(\overrightarrow{X}\)</span>方向的夹角，<em>绝对坐标系下</em></p><p><span class="math inline">\(\delta_{f}\)</span>："前轮转角"，<span class="math inline">\(\delta_{r}\)</span>："后轮转角"，因为轮胎不是纯钢体，所以会有侧偏。<em>车身坐标系下</em></p><blockquote><p>三个假设：左右对称，轮距可以忽略不计，车的轮子是个刚性的轮子</p></blockquote><h3 id="模型建立">模型建立</h3><figure><img src="/2024/07/04/%E6%8E%A7%E5%88%B6%EF%BC%9A%E4%B8%89%E4%B8%AA%E5%9D%90%E6%A0%87%E7%B3%BB%E4%B8%8E%E8%BF%90%E5%8A%A8%E5%AD%A6%E6%96%B9%E7%A8%8B/image-20240405084822954.png" alt><figcaption>image-20240405084822954</figcaption></figure><p><span class="math inline">\(v_{x}\)</span>：纵向车速，<span class="math inline">\(a_x\)</span>：纵向加速度。</p><p><span class="math inline">\(v_y\)</span>：侧向车速，<span class="math inline">\(a_y\)</span>：侧向加速度。</p><p>建立微分方程组：</p><p>​ 几何关系：运动学模型</p><p>​ 牛顿力学：动力学模型</p><h4 id="运动学模型">运动学模型</h4><figure><img src="/2024/07/04/%E6%8E%A7%E5%88%B6%EF%BC%9A%E4%B8%89%E4%B8%AA%E5%9D%90%E6%A0%87%E7%B3%BB%E4%B8%8E%E8%BF%90%E5%8A%A8%E5%AD%A6%E6%96%B9%E7%A8%8B/image-20240408111206138.png" alt><figcaption>image-20240408111206138</figcaption></figure><p><span class="math display">\[\dot{X}=v\cos(\beta+\phi)\]</span></p><p><span class="math display">\[\dot{Y}=v\sin(\beta+\phi)\]</span></p><p><span class="math display">\[\dot{\phi}=\dfrac{v}{R}\]</span></p><p>上述式子体现了速度<span class="math inline">\(v\)</span>对<span class="math inline">\(\dot{X}，\dot{Y}，\dot{\phi}\)</span> 影响，下面推导横摆角对其的影响</p><p><strong>正弦定理：</strong></p><p>设质心到前轮的距离为<span class="math inline">\(a\)</span>，到后轮的距离为<span class="math inline">\(b\)</span></p><p><span class="math inline">\(\dfrac{a}{\sin(\delta_f-\beta)}=\dfrac{R}{\sin(\frac{\pi}{2}-\delta_f)}\)</span></p><p><span class="math inline">\(\dfrac{b}{\sin(\delta_r+\beta)}=\dfrac{R}{\sin(\dfrac{\pi}{2}-\delta_r)}\)</span></p><p><span class="math inline">\(\dfrac{a}{R}=\dfrac{\sin(\delta_f-\beta)}{\sin(\frac{\pi}{2}-\delta_f)}=\dfrac{\sin\delta_f\cos\beta-\sin\beta\cos\delta_f}{\cos\delta_f}=\tan\delta_f\cos\beta-\sin\beta\)</span></p><p><span class="math inline">\(\dfrac{b}{R}=\dfrac{\sin(\delta_r+\beta)}{\sin(\dfrac{\pi}{2}-\delta_r)}=\dfrac{\sin\delta_r\cos\beta+\sin\beta\cos\delta_r}{\cos\delta_r}=\tan\delta_r\cos\beta+\sin\beta\)</span></p><p>因为质心会随着载重变化，所以选择一种不变的量表示比较好</p><p><span class="math inline">\(\dfrac{a+b}{R}=(\tan\delta_f\cos\beta+\tan\delta_r\cos\beta)\)</span> :arrow_right: <span class="math inline">\(\dfrac{1}{R}=\dfrac{\tan\delta_f\cos\beta+\tan\delta_r\cos\beta}{L}\)</span> 其中 <span class="math inline">\(L=a+b\)</span> 为车的轴距</p><p>因此：运动学方程为</p><p><span class="math display">\[\dot{X}=v\cos(\beta+\phi)\]</span></p><p><span class="math display">\[\dot{Y}=v\sin(\beta+\phi)\]</span></p><p><span class="math display">\[\dot{\phi}=\dfrac{v}{R}=\dfrac{v(\tan\delta_f\cos\beta+\tan\delta_r\cos\beta)}{L}\]</span></p><p>在低速情况下，认为车不会发生侧向滑动（漂移）即<span class="math inline">\(v_{y}\approx0\)</span>，因此，根据模型建立的第一个图，<span class="math inline">\(\beta=\arctan\dfrac{v_y}{y_x}\approx0\)</span></p><p>一般后轮不会转向，所以在低速情况下，<span class="math inline">\(\delta_r\approx0\)</span></p><p>因此可以将运动学方程近似为</p><p><span class="math display">\[\dot{X}=v\cos\phi\]</span></p><p><span class="math display">\[\dot{Y}=v\sin\phi\]</span></p><p><span class="math display">\[\dot{\phi}=\dfrac{v}{R}=\dfrac{v\tan\delta_f}{L}\]</span></p><p>又因为<span class="math inline">\(\beta\approx0\)</span>，所以 横摆角<span class="math inline">\(\approx\)</span>航向角</p><p>其中<span class="math inline">\(\phi\)</span> 为横摆角，<span class="math inline">\(v\)</span>为质心速度</p>]]></content>
    
    
    <categories>
      
      <category>控制</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Apollo</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>控制：动力学方程</title>
    <link href="/2024/07/04/%E6%8E%A7%E5%88%B6%EF%BC%9A%E5%8A%A8%E5%8A%9B%E5%AD%A6%E6%96%B9%E7%A8%8B/"/>
    <url>/2024/07/04/%E6%8E%A7%E5%88%B6%EF%BC%9A%E5%8A%A8%E5%8A%9B%E5%AD%A6%E6%96%B9%E7%A8%8B/</url>
    
    <content type="html"><![CDATA[<p>因为运动学方程假设太多，忽略的很多情况，多用于坐标变换，所以建立的模型并不准确。因此需要推导动力学方程</p><p>动力学方程和运动学方程主要区别是<strong>考虑轮胎特性</strong></p><p>当选取Frenet坐标系时，可以出将横纵向控制进行解耦</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>Hello World</title>
    <link href="/2024/07/04/hello-world/"/>
    <url>/2024/07/04/hello-world/</url>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="quick-start">Quick Start</h2><h3 id="create-a-new-post">Create a new post</h3><div class="code-wrapper"><pre><code class="hljs bash">$ hexo new <span class="hljs-string">&quot;My New Post&quot;</span></code></pre></div><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="run-server">Run server</h3><div class="code-wrapper"><pre><code class="hljs bash">$ hexo server</code></pre></div><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="generate-static-files">Generate static files</h3><div class="code-wrapper"><pre><code class="hljs bash">$ hexo generate</code></pre></div><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="deploy-to-remote-sites">Deploy to remote sites</h3><div class="code-wrapper"><pre><code class="hljs bash">$ hexo deploy</code></pre></div><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>决策规划：Frenet 坐标系</title>
    <link href="/2024/07/04/%E5%86%B3%E7%AD%96%E8%A7%84%E5%88%92%EF%BC%9AFrenet%20%E5%9D%90%E6%A0%87%E7%B3%BB/"/>
    <url>/2024/07/04/%E5%86%B3%E7%AD%96%E8%A7%84%E5%88%92%EF%BC%9AFrenet%20%E5%9D%90%E6%A0%87%E7%B3%BB/</url>
    
    <content type="html"><![CDATA[<p>Frenet 坐标与 Cartesian 坐标的转换</p><p>龙格现象：高次多项式拟合可能出现震荡，慎用高次多项式。要尽可能用分段低次多项式去拟合，而不是高次多项式。</p><p>车一般用：host vehicle 表示</p><figure><img src="/2024/07/04/%E5%86%B3%E7%AD%96%E8%A7%84%E5%88%92%EF%BC%9AFrenet%20%E5%9D%90%E6%A0%87%E7%B3%BB/image-20240408110743797.png" alt><figcaption>image-20240408110743797</figcaption></figure>]]></content>
    
    
    <categories>
      
      <category>决策规划</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>决策规划：动态规划</title>
    <link href="/2024/07/04/%E5%86%B3%E7%AD%96%E8%A7%84%E5%88%92%EF%BC%9A%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/"/>
    <url>/2024/07/04/%E5%86%B3%E7%AD%96%E8%A7%84%E5%88%92%EF%BC%9A%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/</url>
    
    <content type="html"><![CDATA[<p>实现一些避障功能</p><figure><img src="/2024/07/04/%E5%86%B3%E7%AD%96%E8%A7%84%E5%88%92%EF%BC%9A%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/image-20240407192707165.png" alt><figcaption>image-20240407192707165</figcaption></figure><p>用 host 在 referance line 的投影作为坐标原点，建立frenet坐标系，将障碍物投影，生成 SL 图。</p><figure><img src="/2024/07/04/%E5%86%B3%E7%AD%96%E8%A7%84%E5%88%92%EF%BC%9A%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/image-20240407193327234.png" alt><figcaption>image-20240407193327234</figcaption></figure><p>确定规划的起点</p><p>定位得到的 host_x , host_y 投影 reference line，得到 SL 坐标(0 , <span class="math inline">\(l_0\)</span>)，以此点为路径规划的</p>]]></content>
    
    
    <categories>
      
      <category>决策规划</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>决策规划：参考线模块</title>
    <link href="/2024/07/04/%E5%86%B3%E7%AD%96%E8%A7%84%E5%88%92%EF%BC%9A%E5%8F%82%E8%80%83%E7%BA%BF%E6%A8%A1%E5%9D%97/"/>
    <url>/2024/07/04/%E5%86%B3%E7%AD%96%E8%A7%84%E5%88%92%EF%BC%9A%E5%8F%82%E8%80%83%E7%BA%BF%E6%A8%A1%E5%9D%97/</url>
    
    <content type="html"><![CDATA[<p>决策规划总体概览(已有导航路径，决策规划流程)</p><ol type="1"><li>定位+导航：生成参考线</li><li>静态障碍物投影到以参考线为坐标轴的frenet坐标轴上</li><li>决策算法对障碍物做决策（往左绕，往右绕，忽略）,开辟最优凸空间</li><li>规划算法在凸空间中搜索出最优的路径</li><li>后处理，在规划轨迹中选取一个点，坐标转换成Cartesian，输出给控制去跟踪</li></ol><h1 id="参考线">参考线</h1><p>参考线是解决方案，解决导航路径过长，不平滑的问题</p><ol type="1"><li>过长的路径不利于坐标转换</li><li>过长的路径，障碍物投影可能不唯一</li><li>不平滑</li></ol><p>解决方案：截取一小段轨迹做平滑，将平滑后的轨迹作为参考线</p><figure><img src="/2024/07/04/%E5%86%B3%E7%AD%96%E8%A7%84%E5%88%92%EF%BC%9A%E5%8F%82%E8%80%83%E7%BA%BF%E6%A8%A1%E5%9D%97/image-20240405092603705-1716974523139-6.png" alt><figcaption>image-20240405092603705-1716974523139-6</figcaption></figure><p>每个规划周期内，找到车在导航路径中的投影点，以投影点为坐标原点，往后取30米，往前取150米范围内的点，做平滑，平滑后的点的集合成为参考线。</p><h2 id="参考线平滑算法">参考线平滑算法</h2><p><strong>平滑性因素</strong></p><figure><img src="/2024/07/04/%E5%86%B3%E7%AD%96%E8%A7%84%E5%88%92%EF%BC%9A%E5%8F%82%E8%80%83%E7%BA%BF%E6%A8%A1%E5%9D%97/image-20240405093102300.png" alt><figcaption>image-20240405093102300</figcaption></figure><p><span class="math inline">\(|\overrightarrow{P_{1}P_{3}}|\)</span>：衡量平滑与不平滑的标准，<span class="math inline">\(|\overrightarrow{P_{1}P_{3}}|\)</span>越小越平滑。缺点：虽然平滑了，但是几何形状差距大。</p><p><strong>道路几何因素</strong></p><figure><img src="/2024/07/04/%E5%86%B3%E7%AD%96%E8%A7%84%E5%88%92%EF%BC%9A%E5%8F%82%E8%80%83%E7%BA%BF%E6%A8%A1%E5%9D%97/image-20240405093701578.png" alt><figcaption>image-20240405093701578</figcaption></figure><p><span class="math inline">\(|\overrightarrow{P_{1}P_{1r}}|+|\overrightarrow{P_{2}P_{2r}}|+|\overrightarrow{P_{3}P_{3r}}|\)</span>越小，越接近原几何路径。</p><p><strong>长度要尽可能均匀和紧凑</strong></p><figure><img src="/2024/07/04/%E5%86%B3%E7%AD%96%E8%A7%84%E5%88%92%EF%BC%9A%E5%8F%82%E8%80%83%E7%BA%BF%E6%A8%A1%E5%9D%97/image-20240405141613878.png" alt><figcaption>image-20240405141613878</figcaption></figure><p><span class="math inline">\(|\overrightarrow{P_{1}P_{2}}|^2+|\overrightarrow{P_{1}P_{3}}|^2\)</span>越小，越均匀，紧凑。</p><h3 id="代价函数">代价函数</h3><figure><img src="/2024/07/04/%E5%86%B3%E7%AD%96%E8%A7%84%E5%88%92%EF%BC%9A%E5%8F%82%E8%80%83%E7%BA%BF%E6%A8%A1%E5%9D%97/image-20240405173124418.png" alt><figcaption>image-20240405173124418</figcaption></figure><p>已知：<span class="math inline">\(x_{1r}...x_{3r},y_{1r}...y_{3r}\)</span></p><p>未知：<span class="math inline">\(x_{1}...x_{3},y_{1}...y_{3}\)</span></p><p>代价函数：</p><p><span class="math display">\[cost function = \\ w_{1}\{\sum_{i=1}^3(x_i-x_{ir})^2+(y_i-y_{ir})^2\}+\\w_2\{(x_1+x_3-2x_2)^2+(y_1+y_3-2y_2)^2\}+\\w_3\{\sum_{i=1}^2(x_{i+1}-x_i)^2+(y_{i+1}-y_i)^2\}\]</span></p><blockquote><p>①与原路径点相似的代价，②平滑代价，③紧凑代价</p></blockquote><p><strong>平滑代价：</strong></p><p>​ <span class="math inline">\(W_{cost\_smooth}\cdot x^TA^T_1A_1x\ ,A_1size=(2n,2n-4)\)</span></p><p><strong>紧凑代价：</strong></p><p>​ <span class="math inline">\(W_{cost\_length}\cdot x^TA^T_2A_2x\ ,A_2size=(2n,2n-2)\)</span></p><p><strong>几何相似代价：</strong></p><p>​ <span class="math inline">\(W_{cost\_ref}\cdot x^TA_3^TA_3x+f^Tx\)</span></p><blockquote><p><span class="math inline">\(\sum_{i=1}^n(x_{ir}-y_{ir})^2\)</span>是个常数，可以忽略。</p></blockquote><h3 id="二次规划问题">二次规划问题</h3><p>二次规划问题的目标函数是变量的二次函数，而约束条件可以是线性的，一般形式如下：</p><p><span class="math display">\[Minimize: \dfrac{1}{2}x^TQx+c^Tx\]</span></p><p><span class="math display">\[Subject\ to: Ax\leq b ,Ex = d\]</span></p><p>其中：</p><ul><li><span class="math inline">\(x\)</span>是一个<span class="math inline">\(n\)</span>维向量，表示决策变量</li><li><span class="math inline">\(Q\)</span>是一个<span class="math inline">\(n\times n\)</span>的对称矩阵，表示目标函数中变量的二次项系数</li><li><span class="math inline">\(c\)</span>是一个<span class="math inline">\(n\)</span>维向量，表示目标函数中变量的一次项系数</li><li><span class="math inline">\(A\)</span>是一个<span class="math inline">\(m\times n\)</span>的矩阵，<span class="math inline">\(b\)</span>是一个<span class="math inline">\(m\)</span>维向量，定义了<span class="math inline">\(m\)</span>个线性不等式约束</li><li><span class="math inline">\(E\)</span>是一个<span class="math inline">\(p\times n\)</span>的矩阵，<span class="math inline">\(d\)</span>是一个<span class="math inline">\(p\)</span>维向量，定义了<span class="math inline">\(p\)</span>个线性等式约束</li></ul><h4 id="针对参考线">针对参考线</h4><p><strong>二次规划：</strong></p><p>​ <span class="math inline">\(=\dfrac{1}{2}x^THx+f^Tx\)</span></p><p>​ <span class="math inline">\(= x^T\dfrac{H}{2}x+f^Tx\)</span></p><p>此处：<span class="math inline">\(H=2(W_{smooth}A^T_1A_1+W_{length}A^T_2A_2+W_{ref}A^T_3A_3)\)</span></p><p><strong>约束：</strong></p><p>​ <span class="math inline">\(x\)</span>与<span class="math inline">\(x_{ref}\)</span>不要离太远，<span class="math inline">\(|x-x_{ref}|\leq buff\)</span></p><p><strong>曲率约束：</strong></p><p>​ 曲率约束是一种非线性约束，一般与车的最大侧向加速度有关(有些弯过不去)，放到后面考虑。</p><h2 id="fem-smoother平滑算法">Fem smoother平滑算法</h2><p>优点：优化变量较少</p><p>缺点：无法保证曲率是连续的，添加曲率约束求解比较麻烦</p><p>参考线是决策规划的基础和前提，必须要快，采用方案如下：</p><ol type="1"><li>减少规划频率，规划算法每100ms执行一次，控制算法每10ms执行一次</li><li>充分利用上个周期计算的结果</li></ol><h3 id="找匹配点">找匹配点</h3><p><strong>普通算法：</strong>每一个规划周期计算一遍，太慢，导航路径很可能很长，遍历费时</p><p><strong>改进算法：</strong></p><figure><img src="/2024/07/04/%E5%86%B3%E7%AD%96%E8%A7%84%E5%88%92%EF%BC%9A%E5%8F%82%E8%80%83%E7%BA%BF%E6%A8%A1%E5%9D%97/image-20240405233336252.png" alt><figcaption>image-20240405233336252</figcaption></figure><p>以上个周期的 match point 为起点做遍历，一旦有<span class="math inline">\(l_{i+1}&gt;l_i\)</span>，立刻退出遍历，<span class="math inline">\(l_i\)</span>对应的点作为本周期的匹配点</p><p><strong>找匹配点的方向：</strong></p><figure><img src="/2024/07/04/%E5%86%B3%E7%AD%96%E8%A7%84%E5%88%92%EF%BC%9A%E5%8F%82%E8%80%83%E7%BA%BF%E6%A8%A1%E5%9D%97/image-20240405233810840.png" alt><figcaption>image-20240405233810840</figcaption></figure><p>如果车往前，就往前遍历，如果车往后，就往后遍历：</p><p>​ <span class="math inline">\(\overrightarrow{d} \cdot \overrightarrow{\tau}&gt;0\)</span>：向前遍历</p><p>​ <span class="math inline">\(\overrightarrow{d} \cdot \overrightarrow{\tau}&lt;0\)</span>：向后遍历</p><p>​ <span class="math inline">\(|\overrightarrow{d} \cdot \overrightarrow{\tau}|&lt;10^{-3}\)</span>：本周期的match point 等于上个周期的match point</p><blockquote><p>本质有点像梯度下降</p></blockquote><p>此方法可以大大加速找匹配点的算法，但只适用于在上个匹配点结果附近只有一个极小值点。</p><figure><img src="/2024/07/04/%E5%86%B3%E7%AD%96%E8%A7%84%E5%88%92%EF%BC%9A%E5%8F%82%E8%80%83%E7%BA%BF%E6%A8%A1%E5%9D%97/image-20240405235016854.png" alt><figcaption>image-20240405235016854</figcaption></figure><p>但是这种问题一般情况下不会出现，因为规划周期是100ms，即使以50m/s的速度运动，车也只走了5m，5m内道路不太可能出现这么扭曲的几何，一般在上个规划周期的匹配点上只有一个极值点。</p><p>万一真的出现这种情况：</p><figure><img src="/2024/07/04/%E5%86%B3%E7%AD%96%E8%A7%84%E5%88%92%EF%BC%9A%E5%8F%82%E8%80%83%E7%BA%BF%E6%A8%A1%E5%9D%97/image-20240405235345495.png" alt><figcaption>image-20240405235345495</figcaption></figure><p>使用 increase_count 变量记录<span class="math inline">\(l\)</span>增加的次数</p><p>if <span class="math inline">\(l_{i+1}&lt;l_i\)</span> (<span class="math inline">\(l\)</span>在减小)</p><p>​ increase_count = 0</p><p>if <span class="math inline">\(l_{i-1}&lt;l_i\)</span> 且 <span class="math inline">\(l_{i+1}&gt;l_i\)</span> (找到 <span class="math inline">\(l_{i}\)</span> 为极小值)</p><p>​ 存储 <span class="math inline">\(l_{i}\)</span></p><p>if <span class="math inline">\(l_{i+1}&gt;l_i\)</span> (<span class="math inline">\(l\)</span>在减大)</p><p>​ increase_count++</p><p>if increase_count &gt; 10 (意味着检查了10个点都没发现另外的极小值点)</p><p>​ break</p><figure><img src="/2024/07/04/%E5%86%B3%E7%AD%96%E8%A7%84%E5%88%92%EF%BC%9A%E5%8F%82%E8%80%83%E7%BA%BF%E6%A8%A1%E5%9D%97/image-20240406000309848.png" alt><figcaption>image-20240406000309848</figcaption></figure><p>第一次运行判断increase_count大于50个点，不是第一次运行则判断大于3个点</p><p><strong>处理点不够的情况：</strong></p><p>如果后面没有30m或者前面没有150m</p><p>解决办法：保持总长度不变(30+150)，(10+170)，(80+100)</p><p><strong>轨迹拼接：</strong></p><figure><img src="/2024/07/04/%E5%86%B3%E7%AD%96%E8%A7%84%E5%88%92%EF%BC%9A%E5%8F%82%E8%80%83%E7%BA%BF%E6%A8%A1%E5%9D%97/image-20240406122706205.png" alt><figcaption>image-20240406122706205</figcaption></figure><p>每100ms周期平滑的轨迹优大量重复的点，我们只需要将上一个规划周期和多出来的平滑轨迹拼接起来就可以</p><p>具体做法，将本周期和上个周期的参考线作比较，如果一模一样就用上个周期的平滑轨迹。若不一样，计算需要拼接多少个点记为n，</p><p>Fem Smoother 至少需要3个点才能够使用</p>]]></content>
    
    
    <categories>
      
      <category>决策规划</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Apollo</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Pytorch 安装</title>
    <link href="/2024/07/04/PyTorch/"/>
    <url>/2024/07/04/PyTorch/</url>
    
    <content type="html"><![CDATA[<h1 id="pytorch-安装">Pytorch 安装</h1><h1 id="线性模型">线性模型</h1><ol type="1"><li>DataSet :smile:</li><li>Model</li><li>Training</li><li>Inferring</li></ol><ul><li><p>过拟合：学习的时候 把噪声也学习进来了。</p></li><li><p>泛化能力：可以正确识别在测试集中没有的数据。</p></li></ul><blockquote><p>通常是得不到测试集的结果，所以为了防止深度学习过程中的过拟合以及增加模型的泛化能力，需要再次将训练集分为训练集和开发集，但训练集训练完成后，需要放到开发集中进行测试，如果满足要求，再把开发集合并到训练集中，再进行训练。</p></blockquote><ul><li><p>损失模型（loss）:在机器学习中评估的方式</p></li><li><p>平均平方误差（MSE）：</p></li></ul><p><span class="math display">\[cost=\frac{1}{N}\sum_{n=1}^{N}{(\hat{y}_n-y_n)^2}\]</span></p><h1 id="梯度下降法">梯度下降法</h1><p><strong>梯度下降法（Gradient Descent）:</strong></p><p><img src="/2024/07/04/PyTorch/image-20240521214147487.png" alt="image-20240521214147487" style="zoom:25%;"></p><p><span class="math display">\[Gradient = \frac{\partial{cost}}{\partial{w}}\]</span> 倒数的负方向就是<span class="math inline">\(cost\)</span>减小的方向：</p><p>核心算法： <span class="math display">\[\omega=\omega-\alpha\frac{\partial{cost}}{\partial{w}}\]</span></p><ul><li><span class="math inline">\(\alpha\)</span>为学习率</li></ul><blockquote><p>梯度下降法体现了贪心的思想，只找当前最好的选择，如果遇到非凸函数，只能找到局部最优解。</p></blockquote><p>需要说明的是，在神经网络中很难陷入到局部最优点，但是遇到<strong>鞍点</strong>后，梯度计算结果为0，会导致<span class="math inline">\(\omega\)</span>无法增加。</p><blockquote><p>如果发现<span class="math inline">\(cost\)</span>发散，证明训练失败，最有可能的原因是因为学习率<span class="math inline">\(\alpha\)</span>太大。</p></blockquote><p><strong>随机梯度下降法：</strong> <span class="math display">\[Gradient = \frac{\partial{loss}}{\partial{w}}\]</span> <span class="math inline">\(cost\)</span>为所有样本的损失函数，当遇到鞍点时可能会陷住无法推进，将所有样本的损失换成单个样本的损失，利用单个样本的随机噪声可能会推动继续前进。</p><blockquote><p>如果利用随机梯度下降法，下一个样本<span class="math inline">\(\omega\)</span>的计算是需要上一个样本的<span class="math inline">\(\omega\)</span>值，就不能并行计算，而梯度下降法则可以实现并行计算。</p></blockquote><p><strong>批量随机梯度下降法</strong></p><p>作为梯度下降法和随机梯度下降法的折中办法，批量随机梯度下降法将多个样本进行分组，在每个组内进行随机梯度下架法。</p><h1 id="反向传播">反向传播</h1><h1 id="张量-tensor">张量 Tensor</h1><p>Tensor是PyTorch里面最基本的数据类型，可以是标量，向量，矩阵，甚至是高维的Tensor，与Numpy中的多维数组非常类似。Tensor还提供了GPU计算和自动求梯度等很多功能，使得Tensor更加适合深度学习。</p><p>Tensor是一个类，里面有两个非常重要的成员，<code>data</code>和<code>grad</code>：</p><ul><li>data中存放的是权重<span class="math inline">\(\omega\)</span>​</li><li>grad中存放的是损失函数对于<span class="math inline">\(\omega\)</span>的导数：<span class="math inline">\(\displaystyle{\frac{\partial{loss}}{\partial{\omega}}}\)</span></li></ul><h1 id="分类问题">分类问题</h1><p>输出值应该为概率，计算输入值属于输出值每一个分类的概率，所有分类的概率值相加应该等于1，然后将概率值进行排序，去概率最大的那个为最终结果。</p><p>与分类任务不同的是，回归任务是预测一个或者多个连续变量，回归任务的核心是通过学习输入变量和输出变量之间的关系来预测未来的输出值。</p><p>回归任务的特点是：</p><ol type="1"><li>目标变量是连续的，回归任务的输出变量可以取任意实数值。</li><li>回归任务存在损失函数，用来衡量预测值与实际值之间的差距。</li></ol><h2 id="sigmoid-函数">Sigmoid 函数</h2><p>通过<code>sigmoid</code>函数可以将输出值对应到0~1的范围内。 <span class="math display">\[y=\frac{1}{1+e^{-x}}\]</span> 在论文中Sigmoid函数一般简写为<span class="math inline">\(\sigma(x)\)</span></p><h2 id="交叉熵">交叉熵</h2><p><span class="math display">\[loss=-(y\log \hat{y}+(1-y)log(1-\hat{y}))\]</span></p><h2 id="加载数据集">加载数据集</h2><ul><li><p>Epoch：所有样本参与一次训练指的是Epoch</p></li><li><p>Batch-Size：一次训练时所用的样本数量</p></li><li><p>Iteration：所有样本所分成的份数，即所有样本数除以Batch-Size数</p></li></ul>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>ROS2：新建流程</title>
    <link href="/2024/07/04/ROS2%20%E6%96%B0%E5%BB%BA%E6%B5%81%E7%A8%8B/"/>
    <url>/2024/07/04/ROS2%20%E6%96%B0%E5%BB%BA%E6%B5%81%E7%A8%8B/</url>
    
    <content type="html"><![CDATA[<h1 id="ros2-新建流程">ROS2 新建流程</h1><h2 id="创建工作空间">1. 创建工作空间</h2><h3 id="创建工作空间以及子目录">1.1 创建工作空间以及子目录</h3><div class="code-wrapper"><pre><code class="hljs Shell">mkdir -p ws00_helloworld/src</code></pre></div><ul><li><code>-p</code>：确保目录名称存在，不存在的就建一个。</li></ul><h3 id="进入工作空间并编译">1.2 进入工作空间并编译</h3><h4 id="进入工作空间">1.2.1 进入工作空间</h4><div class="code-wrapper"><pre><code class="hljs Shell">cd ws00_helloworld</code></pre></div><h4 id="编译">1.2.2 编译</h4><div class="code-wrapper"><pre><code class="hljs Shell">colcon build</code></pre></div><h2 id="创建功能包">2. 创建功能包</h2><p>进入<code>ws00_helloworld/src</code>目录创建功能包</p><div class="code-wrapper"><pre><code class="hljs Shell">ros2 pkg create pkg01_helloworld_cpp --build-type ament_cmake --dependencies rclcpp --node-name helloworld</code></pre></div><h2 id="编辑源文件">3. 编辑源文件</h2><h3 id="vscode-配置">3.1 VSCode 配置</h3><p>换行输入包含路径</p><div class="code-wrapper"><pre><code class="hljs Shell">/opt/ros/foxy/include/**</code></pre></div><ul><li><code>opt</code>： 用来安装附加软件包，是用户级的程序目录</li></ul><h3 id="编辑文件">3.2 编辑文件</h3><h4 id="编辑源文件-1">3.2.1 编辑源文件</h4><div class="code-wrapper"><pre><code class="hljs C++"><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&quot;rclcpp/rclcpp.hpp&quot;</span></span><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">(<span class="hljs-type">int</span> argc, <span class="hljs-type">char</span> ** argv)</span></span><span class="hljs-function"></span>&#123;  rclcpp::<span class="hljs-built_in">init</span>(argc,argv);  <span class="hljs-keyword">auto</span> node = rclcpp::Node::<span class="hljs-built_in">make_shared</span>(<span class="hljs-string">&quot;helloworld&quot;</span>);  <span class="hljs-built_in">RCLCPP_INFO</span>(node-&gt;<span class="hljs-built_in">get_logger</span>(),<span class="hljs-string">&quot;helloworld_node&quot;</span>);  rclcpp::<span class="hljs-built_in">shutdown</span>();  <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;&#125;</code></pre></div><h4 id="编辑-package.xml-文件">3.2.2 编辑 <code>package.xml</code> 文件</h4><h4 id="编辑-cmakelists.txt-文件">3.2.3 编辑 <code>CMakeLists.txt</code> 文件</h4><h2 id="编译-1">4. 编译</h2><p>编译所有功能包：</p><div class="code-wrapper"><pre><code class="hljs Shell">colcon build</code></pre></div><p>编译单个功能包：</p><div class="code-wrapper"><pre><code class="hljs Shell">colcon build --packages-select &lt;name-of-pkg&gt;</code></pre></div><p>编译指定功能包及其依赖：</p><div class="code-wrapper"><pre><code class="hljs Shell">colcon build --packages-up-to &lt;name-of-pkg&gt;</code></pre></div><h2 id="执行">5. 执行</h2><p>进入到工作空间，添加环境变量</p><div class="code-wrapper"><pre><code class="hljs Shell">. install/setup.bash</code></pre></div><ul><li>该命令设置的环境变量只能在该终端中生效</li></ul><p>运行</p><div class="code-wrapper"><pre><code class="hljs Shell">ros2 run pkg01_helloworld_cpp helloworld</code></pre></div>]]></content>
    
    
    <categories>
      
      <category>ROS</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>算法与数据结构</title>
    <link href="/2024/07/02/%E7%AE%97%E6%B3%95%E4%B8%8E%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"/>
    <url>/2024/07/02/%E7%AE%97%E6%B3%95%E4%B8%8E%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/</url>
    
    <content type="html"><![CDATA[<h1 id="算法与数据结构">算法与数据结构</h1><h2 id="认识复杂度对数器二分法">认识复杂度、对数器、二分法</h2><h3 id="常数操作">常数操作</h3><p><strong>基础运算</strong>（+、-、*、/）每一种运算所用的时间是相同的</p><p>例如1+1和54445486+522424242所用的时间应该是相同的，因为都是运行了<code>int</code>型的位数</p><p><strong>寻址操作</strong> 所用的时间是差不多的</p><p>即使是数组，也只需要起始地址和偏移量即可</p><blockquote><p>在单链表中的寻址操作所用的时间是不一样的（不是一个连续结构，所以不能使用偏移量）</p></blockquote><p>常见的常数时间的操作：</p><ul><li>常见的算数运算（+、-、*、/、%）</li><li>常见的位运算（&gt;&gt;、&lt;&lt;、&gt;&gt;&gt;、|、&amp;、^）</li><li>赋值、比较、自增、自减操作</li><li>数组寻址操作</li></ul><h3 id="复杂度">复杂度</h3><h4 id="时间复杂度">时间复杂度</h4><p>时间复杂度：数据量为N的样本中，常数操作的次数的表达式的<strong>最高阶项</strong>。</p><p><span class="math display">\[ax^2+bx+c\]</span>个常数操作的时间复杂度就是<span class="math inline">\(O(n^2)\)</span></p><p>需要把算法流程分解成<strong>常数操作<span class="math inline">\(O(1)\)</span></strong></p><p><span class="math inline">\(O\)</span>指的是最差的数据情况下的时间复杂度</p><h4 id="额外空间复杂度">额外空间复杂度</h4><p>除了样本空间和要求空间外，算法额外开辟的空间（自主空间）</p><h4 id="常数项复杂度">常数项复杂度</h4><ul><li>放弃理论分析，生成随机数据直接测试</li><li>不是不能理论分析，而是没有必要，因为即使是固定常数时间，但是虽然都是常数操作但是，时间上也是不一样的，<em>比如<code>+</code>法运算没有<code>^</code>运算快，<code>/</code></em>不如<code>+</code>快。</li></ul><h4 id="最优解">最优解</h4><ol type="1"><li>时间复杂度尽可能低</li><li>在时间复杂度尽可能低的情况下，空间复杂度尽可能低</li></ol><h4 id="常见时间复杂度排行">常见时间复杂度排行</h4><ol type="1"><li><span class="math inline">\(O(1)\)</span></li><li><span class="math inline">\(O(logN)\)</span></li><li><span class="math inline">\(O(N)\)</span></li><li><span class="math inline">\(O(N*logN)\)</span></li><li><span class="math inline">\(O(N^2)\)</span>，<span class="math inline">\(O(N^3)\)</span>，<span class="math inline">\(O(N^k)\)</span></li><li><span class="math inline">\(O(2^N)\)</span>，<span class="math inline">\(O(3^N)\)</span></li><li><span class="math inline">\(O(k^N)\)</span></li><li><span class="math inline">\(O(!N)\)</span></li></ol><h3 id="对数器">对数器</h3><p>用时间复杂度高的差的办法来测试最优解的办法，差的办法和好的办法是都有可能出错的两套<strong>不同思路</strong>的算法，当两种算法的结果一样，即可验证。</p><p>当出现不一样的结果出现时，去锁定样本数据然后用手工的方法锁定被忽略的边界条件。</p><h3 id="二分法">二分法</h3><p>在有序数组中找到某个元素。样本数变化规律为 <span class="math display">\[N\rightarrow\frac{N}{2}\rightarrow\frac{N}{4}...1\]</span> 每一次找中间值为<span class="math inline">\(O(1)\)</span>，一共有<span class="math inline">\(N\)</span>个样本，每次取一半，时间复杂度为<span class="math inline">\(O(log_2N)\)</span>。</p><p>:bulb:tips：</p><p>中点为<span class="math inline">\(mid=\dfrac{L+R}{2}\)</span></p><p>此时，<span class="math inline">\(L+R\)</span> 可能会溢出，为了避免溢出，可以写成如下公式</p><p><span class="math inline">\(mid=L+\dfrac{R-L}{2}\)</span></p><p>因为移位比除法更快，所以右移一位，写成代码为</p><div class="code-wrapper"><pre><code class="hljs C++">mid = L + ((R + L) &gt;&gt; <span class="hljs-number">1</span>);</code></pre></div><h3 id="插入排序">插入排序</h3><h3 id="冒泡排序">冒泡排序</h3><h3 id="选择排序">选择排序</h3><p><strong>思路：</strong></p><ol type="1"><li>第一轮：找到最小的元素，跟数组第一个数交换</li><li>第二轮：找到除去第一个数剩余元素的最小元素，跟数组的第二个数交换</li><li>直到最后一个元素，排序完成</li></ol><p><strong>时间复杂度：</strong>O(n^2)</p><p><strong>空间复杂度：</strong>O(1)</p><p><strong>算法稳定性：</strong>不稳定</p><div class="code-wrapper"><pre><code class="hljs C++"><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;iostream&gt;</span></span><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;vector&gt;</span></span><span class="hljs-keyword">using</span> <span class="hljs-keyword">namespace</span> std;<span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">selectionSort</span><span class="hljs-params">(vector&lt;<span class="hljs-type">int</span>&gt; &amp;nums)</span> </span>&#123;  <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>; i &lt; nums.<span class="hljs-built_in">size</span>(); ++i) &#123;    <span class="hljs-type">int</span> min_index = i;    <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> j = i + <span class="hljs-number">1</span>; j &lt; nums.<span class="hljs-built_in">size</span>(); ++j) &#123;      <span class="hljs-keyword">if</span> (nums[j] &lt; nums[min_index])        min_index = j;    &#125;    <span class="hljs-keyword">if</span> (min_index != i)      <span class="hljs-built_in">swap</span>(nums[i], nums[min_index]);  &#125;  <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> e : nums)    cout &lt;&lt; e &lt;&lt; <span class="hljs-string">&quot; &quot;</span>;&#125;<span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span> </span>&#123;  <span class="hljs-type">int</span> N;  cin &gt;&gt; N;  <span class="hljs-type">int</span> input_num;  vector&lt;<span class="hljs-type">int</span>&gt; nums;  <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>; i &lt; N; ++i) &#123;    cin &gt;&gt; input_num;    nums.<span class="hljs-built_in">push_back</span>(input_num);  &#125;  <span class="hljs-built_in">selectionSort</span>(nums);  <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;&#125;</code></pre></div><figure><img src="/2024/07/02/%E7%AE%97%E6%B3%95%E4%B8%8E%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/选择排序.gif" alt><figcaption>选择排序</figcaption></figure>]]></content>
    
    
    
    <tags>
      
      <tag>算法</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>深度学习（二）</title>
    <link href="/2024/06/11/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%BA%8C%EF%BC%89/"/>
    <url>/2024/06/11/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%BA%8C%EF%BC%89/</url>
    
    <content type="html"><![CDATA[<h1 id="深度学习二">深度学习（二）</h1><h2 id="线性回归">线性回归</h2><p>线性模型可以看成是一个单层的神经网络</p><p><img src="/2024/06/11/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%BA%8C%EF%BC%89/image-20240611112239112.png" alt="image-20240611112239112" style="zoom: 50%;"></p><ul><li>输入的维度是d</li><li>输出维度是1</li><li>每一个箭头都代表一个权重</li></ul><h3 id="衡量预估质量">衡量预估质量</h3><p>假设<span class="math inline">\(y\)</span>是真实值，<span class="math inline">\(\hat{y}\)</span>是估计值，可以比较 <span class="math display">\[\ell(y,\hat{y})=\frac12\left(y-\hat{y}\right)^2\]</span> 叫做<strong>平方损失</strong></p><blockquote><p>里面的<span class="math inline">\(\frac{1}{2}\)</span>是为了求导的的时候方便消去系数</p></blockquote><h3 id="训练数据">训练数据</h3><ul><li><p>手机一些数据点来决定参数值，这些点被称为训练数据</p></li><li><p>通常越多越好</p></li><li><p>假设有n个样本，记： <span class="math display">\[\mathbf{X}=\begin{bmatrix}\mathbf{x}_1,\mathbf{x}_2,...,\mathbf{x}_n\end{bmatrix}^T\quad\mathbf{y}=\begin{bmatrix}y_1,y_2,...,y_n\end{bmatrix}^T\]</span></p></li></ul><blockquote><p>其中<span class="math inline">\(\mathbf{x}\)</span>代表的是向量，y代表的是标量</p></blockquote><h3 id="参数学习">参数学习</h3><h4 id="训练损失">训练损失</h4><p><span class="math display">\[\ell(\mathbf{X},\mathbf{y},\mathbf{w},b)=\frac1{2n}\sum_{i=1}^n\left(y_i-\langle\mathbf{x}_i,\mathbf{w}\rangle-b\right)^2=\frac1{2n}\left\|\mathbf{y}-\mathbf{X}\mathbf{w}-b\right\|^2\]</span></p><h4 id="最小化损失来学习参数">最小化损失来学习参数</h4><p><span class="math display">\[\mathbf{w}^*,\mathbf{b}^*=\arg\min_{\mathbf{w},b}\ell(\mathbf{X},\mathbf{y},\mathbf{w},b)\]</span></p><ul><li>线性回归是对n维输入的加权，外加偏差</li><li>使用平方损失来衡量预测值和真实值的差异</li><li>线性回归有显示解</li><li>线性回归可以看作是单层神经网络</li></ul><h3 id="基础优化方法">基础优化方法</h3><h4 id="梯度下降">梯度下降</h4><ol type="1"><li><p>挑选一个初始值<span class="math inline">\(w_0\)</span></p></li><li><p>重复迭代参数<span class="math inline">\(t=1,2,3...\)</span> <span class="math display">\[\mathbf{w}_t=\mathbf{w}_{t-1}-\eta\frac{\partial t}{\partial\mathbf{w}_{t-1}}\]</span></p></li><li><p>沿梯度方向将增加损失函数值</p></li><li><p>学习率：步长的超参数</p></li></ol><h4 id="小批量随机梯度下降">小批量随机梯度下降</h4><ul><li><p>在整个训练集上算梯度太贵</p></li><li><p>可以随机采样b个样本<span class="math inline">\(i_1,i_2,...,i_b\)</span>来近似损失 <span class="math display">\[\frac1b\sum_{i\in I_b}\ell(\mathbf{x}_i,y_i,\mathbf{w})\]</span></p></li><li><p>b是批量大小，另一个重要的超参数</p></li></ul><h3 id="线性回归实现">线性回归实现</h3><h2 id="从回归到多类分类">从回归到多类分类</h2><p><strong>回归：</strong></p><p><img src="/2024/06/11/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%BA%8C%EF%BC%89/image-20240611112239112.png" alt="image-20240611112239112" style="zoom: 50%;"></p><ul><li>单连续数值输出</li><li>自然区间R</li><li>跟真实值的区别作为损失</li></ul><p><strong>分类：</strong></p><p><img src="/2024/06/11/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%BA%8C%EF%BC%89/image-20240611112259194.png" alt="image-20240611112259194" style="zoom:50%;"></p><ul><li>通常多个输出</li><li>输出i是预测为第i类的置信度</li></ul><h3 id="softmax-回归">Softmax 回归</h3><ol type="1"><li>对类别进行一位有效编码</li></ol><p><span class="math display">\[\begin{aligned}&amp;\mathbf{y}=[y_1,y_2,...,y_n]^\top\\&amp;y_i=\begin{cases}1\text{ if }i=y\\0\text{ otherwise}&amp;\end{cases}\end{aligned}\]</span></p><ol start="2" type="1"><li><p>使用均方损失训练</p></li><li><p>最大值为预测<span class="math inline">\(\hat{y}=\arg\max o_i\)</span></p></li><li><p>需要更置信的识别正确类（大余量）<span class="math inline">\(o_y-o_i\geq\Delta(y,i)\)</span></p></li><li><p>输出匹配概率（非负，和为1） <span class="math display">\[\hat{\mathbf{y}}=\mathrm{softmax}(\mathbf{o})\\\hat{y}_i=\frac{\exp(o_i)}{\sum_k\exp(o_k)}\]</span></p></li><li><p>概率<span class="math inline">\(y\)</span>和<span class="math inline">\(\hat{y}\)</span>的区别作为损失</p></li><li><p>交叉熵常用来衡量两个概率的区别<span class="math inline">\(H(\mathbf{p},\mathbf{q})=\sum_i-p_i\log(q_i)\)</span></p></li><li><p>将他作为损失<span class="math inline">\(l(\mathbf{y},\mathbf{\hat{y}})=-\sum_iy_i\log\hat{y}_i=-\log\hat{y}_y\)</span></p></li><li><p>其梯度是真实概率和预测概率的区别<span class="math inline">\(\partial_{o_i}l(\mathbf{y},\mathbf{\hat{y}})=\mathrm{softmax}(\mathbf{0})_i-y_i\)</span></p></li></ol><h3 id="损失函数">损失函数</h3><h4 id="l2-loss-均方损失">L2 Loss 均方损失</h4><p><span class="math display">\[l(y,y&#39;)=\frac12(y-y&#39;)^2\]</span></p><blockquote><p>当预测值跟真实值离得越远，权重更新越大</p></blockquote><h4 id="l1-loss-绝对值损失">L1 Loss 绝对值损失</h4><p><span class="math display">\[l(y,y^\prime)=|y-y^\prime|\]</span></p><blockquote><p>权重更新幅度始终一直，不会根据预测值跟真实值差的多少变化，优点是稳定，缺点是在0点处剧烈震荡</p></blockquote><h4 id="hubers-robust-loss-鲁棒损失">Huber`s Robust Loss 鲁棒损失</h4><p><span class="math display">\[l(y,y&#39;)=\begin{cases}|y-y&#39;|-\frac{1}{2}&amp;\text{if}|y-y&#39;|&gt;1\\\frac{1}{2}(y-y&#39;)^2&amp;\text{otherwise}\end{cases}\]</span></p><blockquote><p>均方误差和绝对值误差结合</p></blockquote><h2 id="图像分类数据集">图像分类数据集</h2>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>LSS:Lift, Splat, Shoot 论文代码复现</title>
    <link href="/2024/06/10/LSS%20Lift,%20Splat,%20Shoot%20%E8%AE%BA%E6%96%87%E4%B8%8E%E4%BB%A3%E7%A0%81%E5%A4%8D%E7%8E%B0/"/>
    <url>/2024/06/10/LSS%20Lift,%20Splat,%20Shoot%20%E8%AE%BA%E6%96%87%E4%B8%8E%E4%BB%A3%E7%A0%81%E5%A4%8D%E7%8E%B0/</url>
    
    <content type="html"><![CDATA[<h1 id="lss-lift-splat-shoot-论文代码复现">LSS: Lift, Splat, Shoot 论文代码复现</h1><p><a href="https://arxiv.org/pdf/2008.05711v1.pdf">论文地址</a></p><p><a href="https://github.com/nv-tlabs/lift-splat-shoot">代码地址</a></p><h2 id="环境搭建">环境搭建</h2><p>使用<code>anaconda</code>创建虚拟环境</p><div class="code-wrapper"><pre><code class="hljs shell">conda create -n LSS python=3.8</code></pre></div><p>激活环境</p><div class="code-wrapper"><pre><code class="hljs shell">conda acticate LSS</code></pre></div><p>安装pytorch</p><div class="code-wrapper"><pre><code class="hljs shell">pip install torch-1.9.0+cu102-cp38-cp38-linux_x86_64.whlpip install torchvision-0.10.0+cu102-cp38-cp38-linux_x86_64.whl</code></pre></div><blockquote><p>为了节省时间和流量我一般下载离线的安装包，当然也可以直接按照官网中的在线安装</p></blockquote><p>安装所需要的其他工具包</p><div class="code-wrapper"><pre><code class="hljs shell">pip install nuscenes-devkit tensorboardX efficientnet_pytorch==0.7.0</code></pre></div><h2 id="数据集准备">数据集准备</h2><h3 id="数据集下载">数据集下载</h3><p><a href="https://www.nuscenes.org/download">nuscenes</a></p><p>需要下载<code>Full dataset</code>和<code>Map expansion</code></p><p><img src="/2024/06/10/LSS%20Lift,%20Splat,%20Shoot%20%E8%AE%BA%E6%96%87%E4%B8%8E%E4%BB%A3%E7%A0%81%E5%A4%8D%E7%8E%B0/image-20240610215515414.png" alt="image-20240610215515414" style="zoom:50%;"></p><p><img src="/2024/06/10/LSS%20Lift,%20Splat,%20Shoot%20%E8%AE%BA%E6%96%87%E4%B8%8E%E4%BB%A3%E7%A0%81%E5%A4%8D%E7%8E%B0/image-20240610215717582.png" alt="image-20240610215717582" style="zoom:50%;"></p><h3 id="数据集准备-1">数据集准备</h3><ol type="1"><li><p>在git clone下来的工程目录下创建<code>data</code>文件夹，并将<code>Full dataset</code>压缩包里的文件解压到在这里</p></li><li><p><font color="red">将解压得到的文件夹重命名为<code>mini</code></font></p></li><li><p>将<code>Map expansion</code>压缩包里的文件解压到刚重命名的<code>mini</code>文件夹下的<code>map</code>目录</p></li></ol><h2 id="代码运行">代码运行</h2><p>下载模型文件到任意位置</p><p><a href="https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/efficientnet-b0-355c32eb.pth">模型文件</a></p><p>能够正确运行的目录结构大概如下</p><div class="code-wrapper"><pre><code class="hljs shell">.├── data│   └── mini│       ├── LICENSE│       ├── maps│       ├── samples│       ├── sweeps│       └── v1.0-mini├── efficientnet-b0-355c32eb.pth├── imgs│   ├── check.gif│   └── eval.gif├── LICENSE├── main.py├── README.md├── requirment.txt└── src    ├── data.py    ├── explore.py    ├── __init__.py    ├── models.py    ├── __pycache__    │   ├── data.cpython-38.pyc    │   ├── explore.cpython-38.pyc    │   ├── __init__.cpython-38.pyc    │   ├── models.cpython-38.pyc    │   ├── tools.cpython-38.pyc    │   └── train.cpython-38.pyc    ├── tools.py    └── train.py</code></pre></div><h3 id="evaluate-a-model评估模型">Evaluate a model(评估模型)</h3><div class="code-wrapper"><pre><code class="hljs shell">python main.py eval_model_iou mini --modelf=./efficientnet-b0-355c32eb.pth --dataroot=./data/ --gpuid=0</code></pre></div><ul><li><code>modelf</code> 参数为下载的模型地址</li><li><code>dataroot</code>参数为数据集的地址，也就是<code>mini</code>文件夹所在的位置</li><li><code>gpuid</code>如果没有多个GPU就为0</li></ul><p>如果遇到如下错误，删除<code>int</code>即可</p><div class="code-wrapper"><pre><code class="hljs shell">ImportError: cannot import name &#x27;int&#x27; from &#x27;numpy&#x27; (/home/wang/.conda/envs/LSS/lib/python3.8/site-packages/numpy/__init__.py)</code></pre></div><p>如果遇到如下错误，导致<code>Missing key(s) in state_dict:</code></p><div class="code-wrapper"><pre><code class="hljs shell">Traceback (most recent call last):  File &quot;main.py&quot;, line 13, in &lt;module&gt;    Fire(&#123;  File &quot;/home/wang/.conda/envs/LSS/lib/python3.8/site-packages/fire/core.py&quot;, line 143, in Fire    component_trace = _Fire(component, args, parsed_flag_args, context, name)  File &quot;/home/wang/.conda/envs/LSS/lib/python3.8/site-packages/fire/core.py&quot;, line 477, in _Fire    component, remaining_args = _CallAndUpdateTrace(  File &quot;/home/wang/.conda/envs/LSS/lib/python3.8/site-packages/fire/core.py&quot;, line 693, in _CallAndUpdateTrace    component = fn(*varargs, **kwargs)  File &quot;/home/wang/Project/BEV/lift-splat-shoot/src/explore.py&quot;, line 239, in eval_model_iou    model.load_state_dict(torch.load(modelf))  File &quot;/home/wang/.conda/envs/LSS/lib/python3.8/site-packages/torch/nn/modules/module.py&quot;, line 2189, in load_state_dict    raise RuntimeError(&#x27;Error(s) in loading state_dict for &#123;&#125;:\n\t&#123;&#125;&#x27;.format(RuntimeError: Error(s) in loading state_dict for LiftSplatShoot:</code></pre></div><p>解决办法是：定位到上面报错的这一行</p><div class="code-wrapper"><pre><code class="hljs shell">File &quot;/home/wang/Project/BEV/lift-splat-shoot/src/explore.py&quot;, line 239, in eval_model_iou    model.load_state_dict(torch.load(modelf))</code></pre></div><p>将<code>model.load_state_dict(torch.load(*modelf*))</code>，改为</p><div class="code-wrapper"><pre><code class="hljs python">model.load_state_dict(torch.load(modelf), <span class="hljs-literal">False</span>)</code></pre></div><p>然后再次运行，输出以下内容成功</p><div class="code-wrapper"><pre><code class="hljs shell">running eval...&#123;&#x27;loss&#x27;: 0.7084423531720667, &#x27;iou&#x27;: 0.03712283950617284&#125;</code></pre></div><h3 id="visualize-predictions可视化预测">Visualize Predictions(可视化预测)</h3><div class="code-wrapper"><pre><code class="hljs shell">python main.py viz_model_preds mini --modelf=./efficientnet-b0-355c32eb.pth --dataroot=./data/ --map_folder=./data/mini/ --gpuid=0</code></pre></div><p>会遇到与之前一样的错误，找到<code>model.load_state_dict(torch.load(*modelf*))</code>，跟之前的改法一样。</p><p>运行成功的结果如下</p><div class="code-wrapper"><pre><code class="hljs shell">loading ./efficientnet-b0-355c32eb.pthsaving eval000000_000.jpgsaving eval000000_001.jpg</code></pre></div><p>并有合成的bev效果图如下</p><p><img src="/2024/06/10/LSS%20Lift,%20Splat,%20Shoot%20%E8%AE%BA%E6%96%87%E4%B8%8E%E4%BB%A3%E7%A0%81%E5%A4%8D%E7%8E%B0/eval000000_000.jpg" alt="eval000000_000" style="zoom: 50%;"></p>]]></content>
    
    
    <categories>
      
      <category>BEV</category>
      
    </categories>
    
    
    <tags>
      
      <tag>LSS</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>视觉slam简单入门</title>
    <link href="/2024/06/09/%E8%A7%86%E8%A7%89slam%E7%AE%80%E5%8D%95%E5%85%A5%E9%97%A8/"/>
    <url>/2024/06/09/%E8%A7%86%E8%A7%89slam%E7%AE%80%E5%8D%95%E5%85%A5%E9%97%A8/</url>
    
    <content type="html"><![CDATA[<h1 id="视觉slam简单入门">视觉slam简单入门</h1><ul><li>基于特征点：ORB-Slam</li><li>基于直接法</li></ul><p>解决的问题：</p><ul><li>定位</li><li>地图</li></ul><blockquote><p>定位和地图是相互耦合的</p></blockquote><h2 id="相机分类">相机分类</h2><ul><li>单目相机</li><li>双目相机</li><li>深度相机</li></ul><p>相机的主要缺点：<strong>没有准确的深度信息</strong></p><figure><img src="/2024/06/09/%E8%A7%86%E8%A7%89slam%E7%AE%80%E5%8D%95%E5%85%A5%E9%97%A8/image-20240609122228867.png" alt><figcaption>image-20240609122228867</figcaption></figure><h2 id="视觉slam结构">视觉slam结构</h2><h4 id="前端">前端：</h4><p>视觉里程计（tracking），局部的运动估计，短时间。解决相邻两帧图像之间，相机是如何运动的，需要了解相机是如何投影的，两个图像之间如何做匹配（基于特征点，基于直接法）</p><h4 id="后端">后端：</h4><p>整个地图长什么样子，回环检测（检测回到原点，消除不断累积误差），长时间的轨迹推断（如何简化成一个最小二乘问题）</p><h2 id="d几何">3D几何</h2><h4 id="旋转">旋转</h4><p>将一个坐标系<span class="math inline">\((e_1,e_2,e_3)\)</span>变为<span class="math inline">\((e_1^\prime,e_1^\prime,e_1^\prime)\)</span>，那么一个固定向量<span class="math inline">\(\alpha(a_1,a_2,a_3)\)</span>在新的坐标系下的坐标推导如下： <span class="math display">\[\left.[\boldsymbol{e}_1,\boldsymbol{e}_2,\boldsymbol{e}_3]\left[\begin{array}{c}a_1\\\\a_2\\\\a_3\end{array}\right.\right]=\left[\boldsymbol{e}_1^{&#39;},\boldsymbol{e}_2^{&#39;},\boldsymbol{e}_3^{&#39;}\right]\left[\begin{array}{c}a_1^{&#39;}\\\\a_2^{&#39;}\\\\a_3^{&#39;}\end{array}\right]\]</span> 然后左乘<span class="math inline">\([e_1^T,e_2^T,e_3^T]^T\)</span>，得到： <span class="math display">\[\left.\left[\begin{array}{c}a_1\\\\a_2\\\\a_3\end{array}\right.\right]=\left[\begin{array}{ccc}e_1^Te_1^{&#39;}&amp;e_1^Te_2^{&#39;}&amp;e_1^Te_3^{&#39;}\\e_2^Te_1^{&#39;}&amp;e_2^Te_2^{&#39;}&amp;e_2^Te_3^{&#39;}\\e_3^Te_1^{&#39;}&amp;e_3^Te_2^{&#39;}&amp;e_3^Te_3^{&#39;}\end{array}\right]\left[\begin{array}{c}a_1^{&#39;}\\a_2^{&#39;}\\a_3^{&#39;}\end{array}\right]\triangleq Ra^{&#39;}\]</span> R是旋转矩阵，具有如下特征：</p><ul><li>R是正交矩阵，取逆的话可以直接取转置</li><li>行列式==+1</li></ul><h2 id="前端-1">前端</h2>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>深度学习（一）</title>
    <link href="/2024/06/06/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%80%EF%BC%89/"/>
    <url>/2024/06/06/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%80%EF%BC%89/</url>
    
    <content type="html"><![CDATA[<blockquote><p>李沐老师动手学深度学习的课程记录</p></blockquote><h2 id="数据操作">数据操作</h2><p>N维数组是机器学习和神经网路的主要数据结构</p><h3 id="创建数组">创建数组</h3><p>要创建一个数组，需要知道数组的如下三个特征。</p><ul><li>形状</li><li>每个元素的数据类型</li><li>每个元素的值</li></ul><h3 id="访问元素">访问元素</h3><p>一个元素：[1,2]，代表取第二行，第三列元素。</p><p>一行元素：[1,:]，代表取第二行所有元素。</p><p>一列元素：[:,1]，代表取第二列所有元素。</p><figure><img src="/2024/06/06/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%80%EF%BC%89/image-20240606200854590.png" alt><figcaption>image-20240606200854590</figcaption></figure><p>子区域：[1:3,1:]，代表取第二到第四行且第二到最后一列的区域，<font color="red">开区间结束</font></p><p>子区域：[::3,::2]，代表从第0行0列起，每三行没两列取数。</p><p><img src="/2024/06/06/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%80%EF%BC%89/image-20240606200903680.png" alt="image-20240606200903680" style="zoom: 50%;"></p><h3 id="代码示例">代码示例</h3><h4 id="张量创建">张量创建</h4><p>张量表示一个数值组成的数组，这个数组可能有多个维度</p><div class="code-wrapper"><pre><code class="hljs python">x = torch.arange(<span class="hljs-number">12</span>)<span class="hljs-built_in">print</span>(x)<span class="hljs-string">&#x27;&#x27;&#x27;</span><span class="hljs-string">tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])</span><span class="hljs-string">&#x27;&#x27;&#x27;</span></code></pre></div><h4 id="获取张量元素总数">获取张量元素总数</h4><p>可以通过张量的<code>shape</code>属性来访问张量的<strong>形状</strong>和张量中元素的总数</p><div class="code-wrapper"><pre><code class="hljs python"><span class="hljs-built_in">print</span>(x.shape)<span class="hljs-built_in">print</span>(x.numel())<span class="hljs-string">&#x27;&#x27;&#x27;</span><span class="hljs-string">torch.Size([12])</span><span class="hljs-string">12</span><span class="hljs-string">&#x27;&#x27;&#x27;</span></code></pre></div><h4 id="更改张量形状">更改张量形状</h4><p>要改变一个张量的形状而不改变元素数量和元素值，可以调用<code>reshape</code>函数</p><div class="code-wrapper"><pre><code class="hljs python">X = x.reshape(<span class="hljs-number">3</span>, <span class="hljs-number">4</span>)<span class="hljs-built_in">print</span>(X)<span class="hljs-string">&#x27;&#x27;&#x27;</span><span class="hljs-string">tensor([[ 0,  1,  2,  3],</span><span class="hljs-string">        [ 4,  5,  6,  7],</span><span class="hljs-string">        [ 8,  9, 10, 11]])</span><span class="hljs-string">&#x27;&#x27;&#x27;</span></code></pre></div><p>使用全0，全1，其他常量或者从特定分布中随机采样的数字</p><div class="code-wrapper"><pre><code class="hljs python">torch.zeros((<span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>))torch.ones((<span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>))</code></pre></div><p>可以通过提供包含数值的Python列表（或嵌套列表）来为所需张量中的每个元素赋予确定值</p><div class="code-wrapper"><pre><code class="hljs python">torch.tensor([[<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>],[<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>]])</code></pre></div><h4 id="张量计算">张量计算</h4><p>常见的标准算数运算符（+，-，*，/，**）都可以被升级为<font color="red">按元素计算</font>。</p><div class="code-wrapper"><pre><code class="hljs python">x = torch.tensor([<span class="hljs-number">1.0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">4</span>, <span class="hljs-number">8</span>])y = torch.tensor([<span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>])<span class="hljs-built_in">print</span>(x + y)<span class="hljs-built_in">print</span>(x - y)<span class="hljs-built_in">print</span>(x * y)<span class="hljs-built_in">print</span>(x / y)<span class="hljs-built_in">print</span>(x**y)<span class="hljs-string">&#x27;&#x27;&#x27;</span><span class="hljs-string">tensor([ 3.,  4.,  6., 10.])</span><span class="hljs-string">tensor([-1.,  0.,  2.,  6.])</span><span class="hljs-string">tensor([ 2.,  4.,  8., 16.])</span><span class="hljs-string">tensor([0.5000, 1.0000, 2.0000, 4.0000])</span><span class="hljs-string">tensor([ 1.,  4., 16., 64.])</span><span class="hljs-string">&#x27;&#x27;&#x27;</span></code></pre></div><h4 id="张量拼接">张量拼接</h4><p>可以把多个张量连接在一起</p><ul><li>dim=0，按行拼接</li><li>dim=1，按列拼接</li></ul><div class="code-wrapper"><pre><code class="hljs python">X = torch.arange(<span class="hljs-number">12</span>, dtype=torch.float32).reshape((<span class="hljs-number">3</span>, <span class="hljs-number">4</span>))Y = torch.tensor([[<span class="hljs-number">2.0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">4</span>, <span class="hljs-number">3</span>], [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>], [<span class="hljs-number">4</span>, <span class="hljs-number">3</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>]])Z1 = torch.cat((X, Y), dim=<span class="hljs-number">0</span>)Z2 = torch.cat((X, Y), dim=<span class="hljs-number">1</span>)<span class="hljs-built_in">print</span>(Z1)<span class="hljs-built_in">print</span>(Z2)<span class="hljs-string">&#x27;&#x27;&#x27;</span><span class="hljs-string">tensor([[ 0.,  1.,  2.,  3.],</span><span class="hljs-string">        [ 4.,  5.,  6.,  7.],</span><span class="hljs-string">        [ 8.,  9., 10., 11.],</span><span class="hljs-string">        [ 2.,  1.,  4.,  3.],</span><span class="hljs-string">        [ 1.,  2.,  3.,  4.],</span><span class="hljs-string">        [ 4.,  3.,  2.,  1.]])</span><span class="hljs-string">tensor([[ 0.,  1.,  2.,  3.,  2.,  1.,  4.,  3.],</span><span class="hljs-string">        [ 4.,  5.,  6.,  7.,  1.,  2.,  3.,  4.],</span><span class="hljs-string">        [ 8.,  9., 10., 11.,  4.,  3.,  2.,  1.]])</span><span class="hljs-string">&#x27;&#x27;&#x27;</span></code></pre></div><h4 id="判断张量是否相同">判断张量是否相同</h4><p>可以通过逻辑运算符构建二元张量，按元素进行判断。</p><div class="code-wrapper"><pre><code class="hljs python"><span class="hljs-built_in">print</span>(X==Y)<span class="hljs-string">&#x27;&#x27;&#x27;</span><span class="hljs-string">tensor([[False,  True, False,  True],</span><span class="hljs-string">        [False, False, False, False],</span><span class="hljs-string">        [False, False, False, False]])</span><span class="hljs-string">&#x27;&#x27;&#x27;</span></code></pre></div><p>可以对张量中的所有元素进行求和产生一个只有一个元素的张量</p><div class="code-wrapper"><pre><code class="hljs python"><span class="hljs-built_in">print</span>(X.<span class="hljs-built_in">sum</span>())<span class="hljs-string">&#x27;&#x27;&#x27;</span><span class="hljs-string">tensor(66.)</span><span class="hljs-string">&#x27;&#x27;&#x27;</span></code></pre></div><h4 id="张量的广播操作">张量的广播操作</h4><p>即使形状不同，仍然可以通过广播机制来执行按元素操作，也是最容易出错的地方。</p><div class="code-wrapper"><pre><code class="hljs python">a = torch.arange(<span class="hljs-number">3</span>).reshape((<span class="hljs-number">3</span>, <span class="hljs-number">1</span>))b = torch.arange(<span class="hljs-number">2</span>).reshape((<span class="hljs-number">1</span>, <span class="hljs-number">2</span>))<span class="hljs-built_in">print</span>(a+b)<span class="hljs-string">&#x27;&#x27;&#x27;</span><span class="hljs-string">tensor([[0, 1],</span><span class="hljs-string">        [1, 2],</span><span class="hljs-string">        [2, 3]])</span><span class="hljs-string">&#x27;&#x27;&#x27;</span></code></pre></div><h4 id="张量的读取与写入">张量的读取与写入</h4><p>可以用<code>[1]</code>选择最后一个元素，可以用<code>[1:3]</code>选择第二行和第三行元素。</p><div class="code-wrapper"><pre><code class="hljs python">X = torch.arange(<span class="hljs-number">12</span>).reshape((<span class="hljs-number">3</span>,<span class="hljs-number">4</span>))<span class="hljs-built_in">print</span>(X[-<span class="hljs-number">1</span>])<span class="hljs-built_in">print</span>(X[<span class="hljs-number">1</span>:<span class="hljs-number">3</span>])<span class="hljs-string">&#x27;&#x27;&#x27;</span><span class="hljs-string">tensor([ 8,  9, 10, 11])</span><span class="hljs-string">tensor([[ 4,  5,  6,  7],</span><span class="hljs-string">        [ 8,  9, 10, 11]])</span><span class="hljs-string">&#x27;&#x27;&#x27;</span></code></pre></div><p>除了读取外，可以通过指定索引来将元素写入矩阵</p><div class="code-wrapper"><pre><code class="hljs python">X[<span class="hljs-number">1</span>,<span class="hljs-number">2</span>] = <span class="hljs-number">9</span><span class="hljs-built_in">print</span>(X)<span class="hljs-string">&#x27;&#x27;&#x27;</span><span class="hljs-string">tensor([[ 0,  1,  2,  3],</span><span class="hljs-string">        [ 4,  5,  9,  7],</span><span class="hljs-string">        [ 8,  9, 10, 11]])</span><span class="hljs-string">&#x27;&#x27;&#x27;</span></code></pre></div><p>为多个元素赋予相同的值，只需要索引所有元素，然后为他们赋值</p><div class="code-wrapper"><pre><code class="hljs python">X[<span class="hljs-number">0</span>:<span class="hljs-number">2</span>,:] = <span class="hljs-number">12</span><span class="hljs-built_in">print</span>(X)<span class="hljs-string">&#x27;&#x27;&#x27;</span><span class="hljs-string">tensor([[12, 12, 12, 12],</span><span class="hljs-string">        [12, 12, 12, 12],</span><span class="hljs-string">        [ 8,  9, 10, 11]])</span><span class="hljs-string">&#x27;&#x27;&#x27;</span></code></pre></div><h4 id="有关张量分配内存的写法">有关张量分配内存的写法</h4><p>运行一些操作可能会导致为新结果分配内存，例如</p><div class="code-wrapper"><pre><code class="hljs python">before = <span class="hljs-built_in">id</span>(Y)Y= Y + X<span class="hljs-built_in">print</span>(<span class="hljs-built_in">id</span>(Y)==before)<span class="hljs-string">&#x27;&#x27;&#x27;</span><span class="hljs-string">False</span><span class="hljs-string">&#x27;&#x27;&#x27;</span></code></pre></div><p>如果想要执行原地操作，可以使用按照元素操作的方法，来改变每一个元素的值。</p><div class="code-wrapper"><pre><code class="hljs python">Z = torch.zeros_like(Y)<span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;id(Z):&quot;</span>,<span class="hljs-built_in">id</span>(Z))Z[:] = X + Y<span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;id(Z):&quot;</span>,<span class="hljs-built_in">id</span>(Z))<span class="hljs-string">&#x27;&#x27;&#x27;</span><span class="hljs-string">id(Z): 2615138742384</span><span class="hljs-string">id(Z): 2615138742384</span><span class="hljs-string">&#x27;&#x27;&#x27;</span></code></pre></div><p>如果在后续的计算中没有重复使用X，我们也可以使用<code>X[:] = X + Y</code>或<code>X+=Y</code>来减少操作的内存开销。</p><div class="code-wrapper"><pre><code class="hljs python">before = <span class="hljs-built_in">id</span>(Y)Y += X<span class="hljs-built_in">print</span>(<span class="hljs-built_in">id</span>(Y) == before)<span class="hljs-string">&#x27;&#x27;&#x27;</span><span class="hljs-string">True</span><span class="hljs-string">&#x27;&#x27;&#x27;</span></code></pre></div><h4 id="张量的转换">张量的转换</h4><p>转换为NumPy张量</p><div class="code-wrapper"><pre><code class="hljs python">A = X.numpy()B = torch.tensor(A)<span class="hljs-built_in">print</span>(<span class="hljs-built_in">type</span>(A),<span class="hljs-built_in">type</span>(B))<span class="hljs-string">&#x27;&#x27;&#x27;</span><span class="hljs-string">&lt;class &#x27;numpy.ndarray&#x27;&gt; &lt;class &#x27;torch.Tensor&#x27;&gt;</span><span class="hljs-string">&#x27;&#x27;&#x27;</span></code></pre></div><p>将大小为1的张量转化为Python标量</p><div class="code-wrapper"><pre><code class="hljs python">a = torch.tensor([<span class="hljs-number">3.5</span>])<span class="hljs-built_in">print</span>(a, a.item(), <span class="hljs-built_in">float</span>(a), <span class="hljs-built_in">int</span>(a))<span class="hljs-string">&#x27;&#x27;&#x27;</span><span class="hljs-string">tensor([3.5000]) 3.5 3.5 3</span><span class="hljs-string">&#x27;&#x27;&#x27;</span></code></pre></div><h2 id="数据预处理">数据预处理</h2><p>创建一个人工数据集，并存储在csv（逗号）分隔值文件</p><div class="code-wrapper"><pre><code class="hljs python">os.makedirs(os.path.join(<span class="hljs-string">&quot;..&quot;</span>, <span class="hljs-string">&quot;data&quot;</span>), exist_ok=<span class="hljs-literal">True</span>)data_file = os.path.join(<span class="hljs-string">&quot;..&quot;</span>, <span class="hljs-string">&quot;data&quot;</span>, <span class="hljs-string">&quot;house_tiny.csv&quot;</span>)<span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(data_file, <span class="hljs-string">&quot;w&quot;</span>) <span class="hljs-keyword">as</span> f:    f.write(<span class="hljs-string">&quot;NumRooms,Alley,Price\n&quot;</span>)    f.write(<span class="hljs-string">&quot;NA,Pave,127500\n&quot;</span>)    f.write(<span class="hljs-string">&quot;2,NA,106000\n&quot;</span>)    f.write(<span class="hljs-string">&quot;4,NA,178100\n&quot;</span>)    f.write(<span class="hljs-string">&quot;NA,NA,140000\n&quot;</span>)</code></pre></div><p>从创建的csv文件中加载原始数据集</p><div class="code-wrapper"><pre><code class="hljs python">data = pd.read_csv(data_file)<span class="hljs-built_in">print</span>(data)<span class="hljs-string">&#x27;&#x27;&#x27;</span><span class="hljs-string">   NumRooms Alley   Price</span><span class="hljs-string">0       NaN  Pave  127500</span><span class="hljs-string">1       2.0   NaN  106000</span><span class="hljs-string">2       4.0   NaN  178100</span><span class="hljs-string">3       NaN   NaN  140000</span><span class="hljs-string">&#x27;&#x27;&#x27;</span></code></pre></div><p>为了处理缺失的数据，典型的方法包括<strong>插值</strong>和<strong>删除</strong>，此处为插值示例</p><div class="code-wrapper"><pre><code class="hljs python">inputs, outputs = data.iloc[:, <span class="hljs-number">0</span>:<span class="hljs-number">2</span>], data.iloc[:, <span class="hljs-number">2</span>]inputs = inputs.fillna(inputs.mean(numeric_only=<span class="hljs-literal">True</span>))<span class="hljs-built_in">print</span>(inputs)<span class="hljs-string">&#x27;&#x27;&#x27;</span><span class="hljs-string">   NumRooms Alley</span><span class="hljs-string">0       3.0  Pave</span><span class="hljs-string">1       2.0   NaN</span><span class="hljs-string">2       4.0   NaN</span><span class="hljs-string">3       3.0   NaN</span><span class="hljs-string">&#x27;&#x27;&#x27;</span></code></pre></div><p>对于<code>inputs</code>中的类别值或者离散值，我们将<code>NaN</code>视为一个类别</p><div class="code-wrapper"><pre><code class="hljs python">inputs = pd.get_dummies(inputs, dummy_na=<span class="hljs-literal">True</span>)<span class="hljs-built_in">print</span>(inputs)<span class="hljs-string">&#x27;&#x27;&#x27;</span><span class="hljs-string">   NumRooms  Alley_Pave  Alley_nan</span><span class="hljs-string">0       3.0        True      False</span><span class="hljs-string">1       2.0       False       True</span><span class="hljs-string">2       4.0       False       True</span><span class="hljs-string">3       3.0       False       True</span><span class="hljs-string">&#x27;&#x27;&#x27;</span></code></pre></div><p>现在<code>inputs</code>和<code>outputs</code>中的所有条目都是数值类型，他们可以转换成张量格式。</p><div class="code-wrapper"><pre><code class="hljs python">x, y = torch.tensor(inputs.values), torch.tensor(outputs.values)<span class="hljs-built_in">print</span>(x)<span class="hljs-built_in">print</span>(y)<span class="hljs-string">&#x27;&#x27;&#x27;</span><span class="hljs-string">tensor([[3., 1., 0.],</span><span class="hljs-string">        [2., 0., 1.],</span><span class="hljs-string">        [4., 0., 1.],</span><span class="hljs-string">        [3., 0., 1.]], dtype=torch.float64)</span><span class="hljs-string">tensor([127500, 106000, 178100, 140000])</span><span class="hljs-string">&#x27;&#x27;&#x27;</span></code></pre></div><h2 id="线性代数">线性代数</h2><h3 id="标量">标量</h3><p>标量由只有一个元素的张量表示</p><div class="code-wrapper"><pre><code class="hljs python">x = torch.tensor([<span class="hljs-number">3.0</span>])y = torch.tensor([<span class="hljs-number">2.0</span>])<span class="hljs-built_in">print</span>(x+y,x*y,x/y,x**y)<span class="hljs-string">&#x27;&#x27;&#x27;</span><span class="hljs-string">tensor([5.]) tensor([6.]) tensor([1.5000]) tensor([9.])</span><span class="hljs-string">&#x27;&#x27;&#x27;</span></code></pre></div><p>可以将向量视为标量值组成的列表，可以通过张量的索引来访问任一元素</p><div class="code-wrapper"><pre><code class="hljs python">x = torch.arange(<span class="hljs-number">4</span>)<span class="hljs-built_in">print</span>(x)<span class="hljs-string">&#x27;&#x27;&#x27;</span><span class="hljs-string">tensor([0, 1, 2, 3])</span><span class="hljs-string">&#x27;&#x27;&#x27;</span></code></pre></div><p>可以访问张量的长度，和张量的形状</p><div class="code-wrapper"><pre><code class="hljs python"><span class="hljs-built_in">print</span>(<span class="hljs-built_in">len</span>(x),x.shape)<span class="hljs-string">&#x27;&#x27;&#x27;</span><span class="hljs-string">4 torch.Size([4])</span><span class="hljs-string">&#x27;&#x27;&#x27;</span></code></pre></div><h3 id="矩阵">矩阵</h3><p>创建一个形状为m×n的矩阵</p><div class="code-wrapper"><pre><code class="hljs python">A = torch.arange(<span class="hljs-number">20</span>).reshape(<span class="hljs-number">5</span>,<span class="hljs-number">4</span>)<span class="hljs-built_in">print</span>(A)<span class="hljs-string">&#x27;&#x27;&#x27;</span><span class="hljs-string">tensor([[ 0,  1,  2,  3],</span><span class="hljs-string">        [ 4,  5,  6,  7],</span><span class="hljs-string">        [ 8,  9, 10, 11],</span><span class="hljs-string">        [12, 13, 14, 15],</span><span class="hljs-string">        [16, 17, 18, 19]])</span><span class="hljs-string">&#x27;&#x27;&#x27;</span></code></pre></div><p>矩阵的转置</p><div class="code-wrapper"><pre><code class="hljs python"><span class="hljs-built_in">print</span>(A.T)<span class="hljs-string">&#x27;&#x27;&#x27;</span><span class="hljs-string">tensor([[ 0,  4,  8, 12, 16],</span><span class="hljs-string">        [ 1,  5,  9, 13, 17],</span><span class="hljs-string">        [ 2,  6, 10, 14, 18],</span><span class="hljs-string">        [ 3,  7, 11, 15, 19]])</span><span class="hljs-string">&#x27;&#x27;&#x27;</span></code></pre></div><p>可以构建具有更多轴的数据结构<strong>(行是最后一维)</strong></p><div class="code-wrapper"><pre><code class="hljs python">A = torch.arange(<span class="hljs-number">40</span>).reshape(<span class="hljs-number">2</span>,<span class="hljs-number">5</span>,<span class="hljs-number">4</span>)<span class="hljs-built_in">print</span>(A,A.shape)<span class="hljs-string">&#x27;&#x27;&#x27;</span><span class="hljs-string">tensor([[[ 0,  1,  2,  3],</span><span class="hljs-string">         [ 4,  5,  6,  7],</span><span class="hljs-string">         [ 8,  9, 10, 11],</span><span class="hljs-string">         [12, 13, 14, 15],</span><span class="hljs-string">         [16, 17, 18, 19]],</span><span class="hljs-string"></span><span class="hljs-string">        [[20, 21, 22, 23],</span><span class="hljs-string">         [24, 25, 26, 27],</span><span class="hljs-string">         [28, 29, 30, 31],</span><span class="hljs-string">         [32, 33, 34, 35],</span><span class="hljs-string">         [36, 37, 38, 39]]]) torch.Size([2, 5, 4])</span><span class="hljs-string">&#x27;&#x27;&#x27;</span></code></pre></div><p>按照指定维度求和</p><div class="code-wrapper"><pre><code class="hljs python">A = torch.arange(<span class="hljs-number">40</span>).reshape(<span class="hljs-number">2</span>,<span class="hljs-number">5</span>,<span class="hljs-number">4</span>)<span class="hljs-built_in">print</span>(A.<span class="hljs-built_in">sum</span>(axis=<span class="hljs-number">0</span>),A.<span class="hljs-built_in">sum</span>(axis=<span class="hljs-number">0</span>).shape)<span class="hljs-string">&#x27;&#x27;&#x27;</span><span class="hljs-string">tensor([[20, 22, 24, 26],</span><span class="hljs-string">        [28, 30, 32, 34],</span><span class="hljs-string">        [36, 38, 40, 42],</span><span class="hljs-string">        [44, 46, 48, 50],</span><span class="hljs-string">        [52, 54, 56, 58]]) torch.Size([5, 4])</span><span class="hljs-string">&#x27;&#x27;&#x27;</span></code></pre></div><div class="code-wrapper"><pre><code class="hljs python">A = torch.arange(<span class="hljs-number">40</span>).reshape(<span class="hljs-number">2</span>,<span class="hljs-number">5</span>,<span class="hljs-number">4</span>)<span class="hljs-built_in">print</span>(A.<span class="hljs-built_in">sum</span>(axis=<span class="hljs-number">1</span>),A.<span class="hljs-built_in">sum</span>(axis=<span class="hljs-number">1</span>).shape)<span class="hljs-string">&#x27;&#x27;&#x27;</span><span class="hljs-string">tensor([[ 40,  45,  50,  55],</span><span class="hljs-string">        [140, 145, 150, 155]]) torch.Size([2, 4])</span><span class="hljs-string">&#x27;&#x27;&#x27;</span></code></pre></div><div class="code-wrapper"><pre><code class="hljs python">A = torch.arange(<span class="hljs-number">40</span>).reshape(<span class="hljs-number">2</span>,<span class="hljs-number">5</span>,<span class="hljs-number">4</span>)<span class="hljs-built_in">print</span>(A.<span class="hljs-built_in">sum</span>(axis=<span class="hljs-number">2</span>),A.<span class="hljs-built_in">sum</span>(axis=<span class="hljs-number">2</span>).shape)<span class="hljs-string">&#x27;&#x27;&#x27;</span><span class="hljs-string">tensor([[  6,  22,  38,  54,  70],</span><span class="hljs-string">        [ 86, 102, 118, 134, 150]]) torch.Size([2, 5])</span><span class="hljs-string">&#x27;&#x27;&#x27;</span></code></pre></div><p>按照两个维度求和</p><div class="code-wrapper"><pre><code class="hljs python">A = torch.arange(<span class="hljs-number">40</span>).reshape(<span class="hljs-number">2</span>,<span class="hljs-number">5</span>,<span class="hljs-number">4</span>)<span class="hljs-built_in">print</span>(A.<span class="hljs-built_in">sum</span>(axis=[<span class="hljs-number">0</span>,<span class="hljs-number">1</span>]),A.<span class="hljs-built_in">sum</span>(axis=[<span class="hljs-number">0</span>,<span class="hljs-number">1</span>]).shape)<span class="hljs-string">&#x27;&#x27;&#x27;</span><span class="hljs-string">tensor([180, 190, 200, 210]) torch.Size([4])</span><span class="hljs-string">&#x27;&#x27;&#x27;</span></code></pre></div><p>计算总和或者均值时保持轴数不变，方便广播机制</p><div class="code-wrapper"><pre><code class="hljs python">A = torch.arange(<span class="hljs-number">40</span>).reshape(<span class="hljs-number">2</span>, <span class="hljs-number">5</span>, <span class="hljs-number">4</span>)<span class="hljs-built_in">print</span>(A.<span class="hljs-built_in">sum</span>(axis=<span class="hljs-number">1</span>, keepdim=<span class="hljs-literal">True</span>), A.<span class="hljs-built_in">sum</span>(axis=<span class="hljs-number">1</span>, keepdim=<span class="hljs-literal">True</span>).shape)<span class="hljs-string">&#x27;&#x27;&#x27;</span><span class="hljs-string">tensor([[[ 40,  45,  50,  55]],</span><span class="hljs-string"></span><span class="hljs-string">        [[140, 145, 150, 155]]]) torch.Size([2, 1, 4])</span><span class="hljs-string">&#x27;&#x27;&#x27;</span></code></pre></div>]]></content>
    
    
    <categories>
      
      <category>深度学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>跟李沐学AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>视觉Slam基础</title>
    <link href="/2024/06/02/%E8%A7%86%E8%A7%89Slam%E5%9F%BA%E7%A1%80/"/>
    <url>/2024/06/02/%E8%A7%86%E8%A7%89Slam%E5%9F%BA%E7%A1%80/</url>
    
    <content type="html"><![CDATA[<h1 id="视觉slam基础">视觉Slam基础</h1><h2 id="相机模型">相机模型</h2><h3 id="单目相机">单目相机</h3><p><img src="/2024/06/02/%E8%A7%86%E8%A7%89Slam%E5%9F%BA%E7%A1%80/image-20240602151348721.png" alt="image-20240602151348721" style="zoom:67%;"></p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>计算机图形学（一）</title>
    <link href="/2024/06/02/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9B%BE%E5%BD%A2%E5%AD%A6%EF%BC%88%E4%B8%80%EF%BC%89/"/>
    <url>/2024/06/02/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9B%BE%E5%BD%A2%E5%AD%A6%EF%BC%88%E4%B8%80%EF%BC%89/</url>
    
    <content type="html"><![CDATA[<h1 id="计算机图形学一">计算机图形学（一）</h1><h2 id="计算机图形学中的变换">计算机图形学中的变换</h2><h3 id="缩放-scale">缩放 Scale</h3><p><span class="math display">\[M=\begin{bmatrix}S_x&amp;0\\0&amp;S_y\end{bmatrix}\]</span></p><ul><li><span class="math inline">\(S_x\)</span>：x轴的缩放因子</li><li><span class="math inline">\(S_y\)</span>：y轴的缩放因子</li></ul><p><img src="/2024/06/02/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9B%BE%E5%BD%A2%E5%AD%A6%EF%BC%88%E4%B8%80%EF%BC%89/image-20240602125036586.png" alt="image-20240602125036586"> <span class="math display">\[\begin{bmatrix}x&#39;\\y&#39;\end{bmatrix}=M\begin{bmatrix}x\\y\end{bmatrix}=\begin{bmatrix}S_x&amp;&amp;0\\0&amp;&amp;S_y\end{bmatrix}\begin{bmatrix}x\\y\end{bmatrix}=\begin{bmatrix}S_x\cdot x\\S_y\cdot y\end{bmatrix}\]</span></p><ul><li>当<span class="math inline">\(S_x\)</span>或者<span class="math inline">\(S_y\)</span>大于1的时候，图片沿着相应方向放大，当小于1的时候，沿着相应方向缩小。</li><li>当<span class="math inline">\(S_x=S_y\)</span>的时候，图片按照比例放大缩小</li></ul><h3 id="旋转-rotate">旋转 Rotate</h3><p><span class="math display">\[R(\theta)=\begin{bmatrix}\cos(\theta)&amp;-\sin(\theta)\\\sin(\theta)&amp;\cos(\theta)\end{bmatrix}\]</span></p><ul><li><span class="math inline">\(\theta\)</span>：逆时针旋转角度，单位是弧度</li></ul><p><img src="/2024/06/02/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9B%BE%E5%BD%A2%E5%AD%A6%EF%BC%88%E4%B8%80%EF%BC%89/image-20240602125357043.png" alt="image-20240602125357043"> <span class="math display">\[\begin{bmatrix}x&#39;\\y&#39;\end{bmatrix}=R(\theta)\begin{bmatrix}x\\y\end{bmatrix}=\begin{bmatrix}\cos(\theta)&amp;-\sin(\theta)\\\sin(\theta)&amp;\cos(\theta)\end{bmatrix}\begin{bmatrix}x\\y\end{bmatrix}=\begin{bmatrix}x\cos(\theta)-y\sin(\theta)\\x\sin(\theta)+y\cos(\theta)\end{bmatrix}\]</span></p><h3 id="平移-homogeneous-coordinatess">平移 Homogeneous Coordinatess</h3><p><span class="math display">\[2\text{D point}=(x,y,1)^T\\2\text{D vector}=(x,y,0)^T\]</span></p><p><span class="math display">\[T=\begin{bmatrix}1&amp;0&amp;T_x\\0&amp;1&amp;T_y\\0&amp;0&amp;1\end{bmatrix}\]</span></p><ul><li><p>齐次坐标允许将平移、旋转、缩放等几何变换统一表示为矩阵乘法。这使得变换组合和计算更加简洁和高效。</p></li><li><p>一个二维点 (x,y)的齐次坐标表示为 (x,y,1)。这个表示方式允许通过矩阵变换进行各种几何操作，如平移、旋转和缩放。</p></li><li><p>一个二维向量 (x,y)的齐次坐标表示为 (x,y,0)。向量的最后一个分量是 0，这表示向量不受平移变换的影响，仅参与旋转和缩放等线性变换。</p></li></ul><p><img src="/2024/06/02/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9B%BE%E5%BD%A2%E5%AD%A6%EF%BC%88%E4%B8%80%EF%BC%89/image-20240602130512876.png" alt="image-20240602130512876"> <span class="math display">\[\begin{bmatrix}x&#39;\\y&#39;\\1\end{bmatrix}=T\begin{bmatrix}x\\y\\1\end{bmatrix}=\begin{bmatrix}1&amp;0&amp;T_x\\0&amp;1&amp;T_y\\0&amp;0&amp;1\end{bmatrix}\begin{bmatrix}x\\y\\1\end{bmatrix}=\begin{bmatrix}x+T_x\\y+T_y\\1\end{bmatrix}\]</span></p><h2 id="视图变换-view-transform">视图变换 View Transform</h2><h3 id="相机轴定义">相机轴定义</h3><p>通常，大家约定：</p><ul><li><p>相机向上的方向为Y轴</p></li><li><p>相机看向的方向为Z轴</p></li><li><p>而X轴则遵循右手定则</p></li></ul><figure><img src="/2024/06/02/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9B%BE%E5%BD%A2%E5%AD%A6%EF%BC%88%E4%B8%80%EF%BC%89/image-20240602131828558.png" alt><figcaption>image-20240602131828558</figcaption></figure><blockquote><p>但是有些论文或者库定义会有所不同，例如ColMap</p></blockquote><h3 id="世界坐标系惯性坐标系">世界坐标系（惯性坐标系）</h3><p>在机器人的运动过程中，常见的做法是设定一个惯性坐标系，可以认为它是固定不动的（相对的），如果是单目相机，会选择拍摄的第一张照片作为实践坐标系，如果是双目摄像机，会选择其中一个作为世界坐标系。</p><h3 id="移动坐标系相机坐标系">移动坐标系（相机坐标系）</h3><p>同时，相机或者机器人则是一个移动坐标系</p><h2 id="投影变换-projection">投影变换 Projection</h2><h3 id="正交变换">正交变换</h3>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>NeRF</title>
    <link href="/2024/05/31/NeRF/"/>
    <url>/2024/05/31/NeRF/</url>
    
    <content type="html"><![CDATA[<h1 id="nerf">NeRF</h1><p>NeRF 就是用一堆二维图像去重建三维场景，可以用虚拟相机在三维场景的不同位置得到渲染好的RGB图像。优点是渲染质量高，深度结果也很准确。</p>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
